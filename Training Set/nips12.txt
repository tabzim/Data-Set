"A generative model for attractor dynamics." Advances in Neural Information Processing Systems. 2000.
Attractor networks, which map an input space to a discrete out- 
put space, are useful for pattern completion. However, designing 
a net to have a given set of attractors is notoriously tricky; training 
procedures are CPU intensive and often produce spurious attrac- 
tors and ill-conditioned attractor basins. These difficulties occur 
because each connection in the network participates in the encod- 
ing of multiple attractors. We describe an alternative formulation 
of attractor networks in which the encoding of knowledge is local, 
not distributed. Although localist attractor networks have similar 
dynamics to their distributed counterparts, they are much easier 
to work with and interpret. We propose a statistical formulation of 
localist attractor net dynamics, which yields a convergence proof 
and a mathematical interpretation of model parameters. 
