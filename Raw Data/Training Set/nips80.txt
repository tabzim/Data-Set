"v-Arc: Ensemble Learning in the Presence of Outliers." Advances in Neural Information Processing Systems. 2000. 
AdaBoost and other ensemble methods have successfully been ap- 
plied to a number of classification tasks, seemingly defying prob- 
lems of overfitting. AdaBoost performs gradient descent in an error 
function with respect to the margin, asymptotically concentrating 
on the patterns which are hardest to learn. For very noisy prob- 
lems, however, this can be disadvantageous. Indeed, theoretical 
analysis has shown that the margin distribution, as opposed to just 
the minimal margin, plays a crucial role in understanding this phe- 
nomenon. Loosely speaking, some outliers should be tolerated if 
this has the benefit of substantially increasing the margin on the 
remaining points. We propose a new boosting algorithm which al- 
lows for the possibility of a pre-specified fraction of points to lie in 
the margin area or even on the wrong side of the decision boundary. 
